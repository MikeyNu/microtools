"use client"

import { useState } from "react"
import { FileText, ArrowLeft, Copy } from "lucide-react"
import { Button } from "@/components/ui/button"
import { Card, CardContent, CardHeader, CardTitle } from "@/components/ui/card"
import { Input } from "@/components/ui/input"
import { Label } from "@/components/ui/label"
import { Textarea } from "@/components/ui/textarea"
import { Checkbox } from "@/components/ui/checkbox"
import { useToast } from "@/hooks/use-toast"
import Link from "next/link"

export default function RobotsGeneratorPage() {
  const [sitemapUrl, setSitemapUrl] = useState("")
  const [allowAll, setAllowAll] = useState(true)
  const [disallowPaths, setDisallowPaths] = useState("")
  const [crawlDelay, setCrawlDelay] = useState("")
  const [customRules, setCustomRules] = useState("")
  const { toast } = useToast()

  const generateRobotsTxt = () => {
    let robotsContent = "# Robots.txt generated by ToolHub\n\n"

    if (allowAll) {
      robotsContent += "User-agent: *\n"
      robotsContent += "Allow: /\n\n"
    } else {
      robotsContent += "User-agent: *\n"
      robotsContent += "Disallow: /\n\n"
    }

    // Add disallow paths
    if (disallowPaths.trim()) {
      const paths = disallowPaths.split("\n").filter((path) => path.trim())
      if (paths.length > 0) {
        robotsContent += "# Disallowed paths\n"
        paths.forEach((path) => {
          robotsContent += `Disallow: ${path.trim()}\n`
        })
        robotsContent += "\n"
      }
    }

    // Add crawl delay
    if (crawlDelay.trim()) {
      robotsContent += `Crawl-delay: ${crawlDelay}\n\n`
    }

    // Add custom rules
    if (customRules.trim()) {
      robotsContent += "# Custom rules\n"
      robotsContent += customRules + "\n\n"
    }

    // Add sitemap
    if (sitemapUrl.trim()) {
      robotsContent += `Sitemap: ${sitemapUrl}\n`
    }

    return robotsContent
  }

  const copyRobotsTxt = () => {
    const robotsContent = generateRobotsTxt()
    navigator.clipboard.writeText(robotsContent)
    toast({
      title: "Copied!",
      description: "Robots.txt content copied to clipboard",
    })
  }

  const downloadRobotsTxt = () => {
    const robotsContent = generateRobotsTxt()
    const blob = new Blob([robotsContent], { type: "text/plain" })
    const url = URL.createObjectURL(blob)
    const link = document.createElement("a")
    link.href = url
    link.download = "robots.txt"
    document.body.appendChild(link)
    link.click()
    document.body.removeChild(link)
    URL.revokeObjectURL(url)

    toast({
      title: "Downloaded!",
      description: "Robots.txt file downloaded successfully",
    })
  }

  return (
    <div className="min-h-screen bg-background">
      {/* Header */}
      <header className="border-b bg-card/50 backdrop-blur-sm">
        <div className="container mx-auto px-4 py-4">
          <div className="flex items-center justify-between">
            <Link href="/" className="flex items-center space-x-2">
              <FileText className="h-8 w-8 text-primary" />
              <h1 className="text-2xl font-serif font-bold text-primary">ToolHub</h1>
            </Link>
            <Link
              href="/seo-tools"
              className="flex items-center space-x-2 text-muted-foreground hover:text-primary transition-colors"
            >
              <ArrowLeft className="h-4 w-4" />
              Back to SEO Tools
            </Link>
          </div>
        </div>
      </header>

      <div className="container mx-auto px-4 py-8">
        <div className="max-w-4xl mx-auto">
          <Card>
            <CardHeader className="text-center">
              <CardTitle className="font-serif text-2xl">Robots.txt Generator</CardTitle>
              <p className="text-muted-foreground">Create robots.txt files to control search engine crawling</p>
            </CardHeader>
            <CardContent className="space-y-6">
              <div className="grid grid-cols-1 lg:grid-cols-2 gap-6">
                <div className="space-y-4">
                  <div className="flex items-center space-x-2">
                    <Checkbox
                      id="allowAll"
                      checked={allowAll}
                      onCheckedChange={(checked) => setAllowAll(checked as boolean)}
                    />
                    <Label htmlFor="allowAll">Allow all web crawlers to access all content</Label>
                  </div>

                  <div>
                    <Label htmlFor="sitemapUrl">Sitemap URL</Label>
                    <Input
                      id="sitemapUrl"
                      placeholder="https://yourwebsite.com/sitemap.xml"
                      value={sitemapUrl}
                      onChange={(e) => setSitemapUrl(e.target.value)}
                    />
                  </div>

                  <div>
                    <Label htmlFor="disallowPaths">Disallow Paths (one per line)</Label>
                    <Textarea
                      id="disallowPaths"
                      placeholder="/admin/&#10;/private/&#10;/temp/"
                      value={disallowPaths}
                      onChange={(e) => setDisallowPaths(e.target.value)}
                      className="min-h-[100px]"
                    />
                  </div>

                  <div>
                    <Label htmlFor="crawlDelay">Crawl Delay (seconds)</Label>
                    <Input
                      id="crawlDelay"
                      type="number"
                      placeholder="10"
                      value={crawlDelay}
                      onChange={(e) => setCrawlDelay(e.target.value)}
                    />
                  </div>

                  <div>
                    <Label htmlFor="customRules">Custom Rules</Label>
                    <Textarea
                      id="customRules"
                      placeholder="User-agent: Googlebot&#10;Allow: /special/"
                      value={customRules}
                      onChange={(e) => setCustomRules(e.target.value)}
                      className="min-h-[80px]"
                    />
                  </div>
                </div>

                <div className="space-y-4">
                  <div className="flex items-center justify-between">
                    <h3 className="font-serif text-lg font-semibold">Generated Robots.txt</h3>
                    <div className="flex space-x-2">
                      <Button variant="outline" size="sm" onClick={copyRobotsTxt}>
                        <Copy className="h-4 w-4 mr-2" />
                        Copy
                      </Button>
                      <Button size="sm" onClick={downloadRobotsTxt}>
                        Download
                      </Button>
                    </div>
                  </div>
                  <Textarea value={generateRobotsTxt()} readOnly className="min-h-[300px] font-mono text-sm" />
                </div>
              </div>

              <div className="bg-muted p-4 rounded-lg">
                <h4 className="font-semibold mb-2">Common Robots.txt Patterns</h4>
                <div className="grid grid-cols-1 md:grid-cols-2 gap-4 text-sm text-muted-foreground">
                  <div>
                    <strong>Block admin areas:</strong> /admin/, /wp-admin/
                  </div>
                  <div>
                    <strong>Block search results:</strong> /search/, /?s=
                  </div>
                  <div>
                    <strong>Block private files:</strong> /private/, /temp/
                  </div>
                  <div>
                    <strong>Block duplicate content:</strong> /tag/, /category/
                  </div>
                </div>
              </div>

              <div className="bg-yellow-50 border border-yellow-200 p-4 rounded-lg">
                <h4 className="font-semibold mb-2 text-yellow-800">Important Notes</h4>
                <ul className="text-sm text-yellow-700 space-y-1">
                  <li>• Place robots.txt in your website's root directory</li>
                  <li>• Test your robots.txt using Google Search Console</li>
                  <li>• Robots.txt is publicly accessible - don't use it to hide sensitive content</li>
                  <li>• Some crawlers may ignore robots.txt directives</li>
                </ul>
              </div>
            </CardContent>
          </Card>
        </div>
      </div>
    </div>
  )
}
